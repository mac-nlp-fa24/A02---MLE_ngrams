{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8201bf23-1c8a-4163-a50a-925c668f9aa4",
   "metadata": {},
   "source": [
    "# N-Grams and Maximum Likelihood Estimation\n",
    "\n",
    "## Quick Review\n",
    "As you've seen in lecture and/or in the reading, n-gram language models are structured around taking a direct, empirical approach to solving the language modeling task: We compute $p(w \\mid C)$ by estimating $p(w, C)$ and $p(C)$ by counting occurances in a large corpus of text. \n",
    "\n",
    "However, as you saw, the context, $C$, can be really long, and we probably don't have enough data (or, at least, data that you can process on your computer) to get a good estimate of $p(w, C)$ and $p(C)$. N-grams models handle this by making a **Markov Assumption**: We assume that we only need a fixed amount of prior context to predict the next word. How much prior context? An $n$-gram model assumes a context of $n-1$ words!\n",
    "\n",
    "That is, we assume $p(w \\mid C) \\approx p(w \\mid w_{-1}, \\dots w_{-n+1})$. If we label our random variables such that a sentence is a series of random variables $W_1, \\dots$, we get the much nicer form $p(w_k \\mid w_1 \\dots w_{k-1}) \\approx p(w_k \\mid w_{k-n} \\dots w_{k-1})$. More casually, $p(\\text{next word} \\mid \\text{prior context}) \\approx p(\\text{next word} \\mid \\text{past $n$ words})$.\n",
    "\n",
    "## This Assignment\n",
    "\n",
    "Our learning goals for this assignment are to...\n",
    "\n",
    "    1. Understand how an n-gram language model is trained and used.\n",
    "    2. Implement an n-gram language model and training using Maximum Likelihood Estimation\n",
    "\n",
    "To start, let's load in some data: the text of Jane Austin's Emma, via Project Gutenberg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c72df6-187b-4bf8-b303-5b5b4b2298af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', '<eos>', 'volume', 'i']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"austen-emma.txt\") as train_f:\n",
    "    train = train_f.read().lower().split()\n",
    "\n",
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba36b42-ea0b-4976-b1b5-a5c17b67906e",
   "metadata": {},
   "source": [
    "You can see that we do some light pre-processing --- we lowercase all of the text. You should also see that the corpus you've been given has been annotated with `\\<eos>` markers. Like the `\\<s>` and `\\<\\s>` tokens you saw in the reading, these tokens serve to mark sentence boundaries. In classical NLP, these markers are often used so that separate sentences are distinct from eachother. In modern, neural NLP, you'll find that such careful preprocessing has largely been abandoned. \n",
    "\n",
    "Before we get to start dealing with n-gram models at all, we need to do some addition, n-gram specific pre-processing on our data. The first task is to get counts for n-grams, and before we get there, it's prudent to be able to split our data into overlapping $n$-word sequences. \n",
    "\n",
    "For example, \"This is a sentence . \\<eos>\" would turn into the list of bigrams, each of which is represented by a tuple \\[(\"this\", \"is\"), (\"is\", \"a\"), (\"a\", \"sentence\"), (\"sentence\", \".\"), (\".\". \"\\<eos>\")]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71a12a1-5c82-43d2-b509-d317bfe01004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence, Iterable\n",
    "\n",
    "def get_ngrams(words : Sequence[str], n : int) -> Iterable[tuple[str]]:\n",
    "    # TODO: Complete this implementation\n",
    "    return []\n",
    "\n",
    "print(list(get_ngrams(\"this is a sentence . <eos>\".split(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1dc5ad-0cb6-49ae-a34e-16dfe97cb86b",
   "metadata": {},
   "source": [
    "Now we could go ahead and start counting ngrams now, but there's one thing to observe: We don't ever get to estimate the probability of the first n-1 words of the text. That is, we never use the fact that \"this\" is the first word in the sentence because there is no bigram where \"this\" is the second word! \n",
    "\n",
    "As you saw in the reading, this calls for *padding* the text. That is, adding a \"boundary\" of several separator tokens so that ngram models can estimate the probability of beginning a sentence/paragraph/text. It was convention in what I'll call \"classical\" language modeling to separate sentences from each other with a sequence of padding tokens, though in modern NLP such conventions have fallen to the wayside due to the scale of data involved.\n",
    "\n",
    "For this assignment, let's pad *sentence by sentence*\n",
    "\n",
    "First, ask yourself:\n",
    "\n",
    "1. For an $n$-gram model, how many padding tokens should be between each sentence? Remember that we want to have an ngram correspond to the likelihood of the first word of the sentence being the first word of the sentence!\n",
    "2. How are sentence boundaries currently marked? How can you use those to create the proper amount of padding tokens?\n",
    "\n",
    "Fill in an algorithm to pad a text in the Gutenberg corpus format of Austen's Emma above. Use `<eos>` as a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63eaa05-722b-4757-bf1b-2ebc5c9b9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(words : Sequence[str]) -> Sequence[str]:\n",
    "    # TODO: Complete this implementation\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9016da57-5d16-492e-b058-613a23a85a2b",
   "metadata": {},
   "source": [
    "Now you have code to roughly preprocess data for language modeling!\n",
    "\n",
    "Of course, you can preprocess *more* if you wish, typically in the interest of avoiding rare or unseen tokens in your vocabulary. Sometimes folks replace all numbers with a single token `<num>` so the model just has to predict a number comes next. Other times, you take all of the super infrequent words and replace them with `<unk>` (unknown) (or `<oov>`/out-of-vocabulary) tokens, and preprocess any data after training to have `<unk>`s instead of any out-of-vocab words. For now, we can hold off on those tricks.  \n",
    "\n",
    "Our next step is to *train* our model. Here, that notion refers to picking a training corpus (our preprocessed Emma) and learning some *parameters* (that is, numbers in our model we don't know a priori) using that corpus. For an n-gram model, those parameters are our counts of each ngram!\n",
    "\n",
    "Complete the function below that does this, and test that it works using a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac99340-2d09-49d1-908f-3bcf7e326f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "def get_counts(train : Sequence[str], n : int) -> Mapping[str, int]:\n",
    "    counts = {}\n",
    "    # TODO: count the occurances of each ngram in train\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d63f0-3b53-4df3-b584-f31f7727b58e",
   "metadata": {},
   "source": [
    "And now we're finally ready to build an ngram model! Below is a template for an mle_ngram_lm (that is, an MLE-trained ngram LM). Complete the two sections marked TODO to complete the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ccbcf55-8540-4322-ba56-9ce93c3496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mle_ngram_lm:\n",
    "    def __init__(self, train : Sequence[str], n : int) -> None:\n",
    "        # TODO: train the model here, declaring appropriate instance variables (i.e., model parameters!)\n",
    "        # Consider what you need to compute logprob(w, c)!\n",
    "        pass\n",
    "\n",
    "    def logprob(self, w : str, c : Sequence[str]) -> float:\n",
    "        assert (len(c) + 1) == self.n\n",
    "        # TODO: compute p(w | c) using those instance variables\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01730ace-cc14-4cf8-b951-d44b2ac25991",
   "metadata": {},
   "source": [
    "When the above code is complete, test the implementation on a small example:\n",
    "\n",
    "1. Write a few sentences.\n",
    "2. Preprocess those sentences.\n",
    "3. **By hand** compute the frequencies of a few unigrams/bigrams/trigrams. Make sure there are a few bigrams/trigrams with probability strictly between 0 and 1!\n",
    "4. Train your model on those sentences and confirm your code's output and your by-hand calculations match!\n",
    "5. Modify your sentences if you've spotted a tricky edge case you can construct, or modify your code (or, perhaps, your by-hand calculations!) if there was a mismatch in (4)!\n",
    "\n",
    "Once you've done this to your satisfaction (your convinced your code is right!), train the model on the full text of Emma in the cell below. Be sure to apply the full pre-processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5066dfe-9f62-4830-aae3-87326f068581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "494dbdf8-f86d-41b4-b604-07fd21e91c89",
   "metadata": {},
   "source": [
    "**Bonus:** Given extra time, consult the reading for a definition of *perplexity*, a measure often used to determine how good a language model is. In the cell below, complete an implementation of perplexity and use it to measure the perplexity of your n-gram model over it's training data. \n",
    "\n",
    "Consider the following questions\n",
    "1. For a given $n$, can any $n$-gram model have a lower (i.e., better!) perplexity over the training set than the one you trained?\n",
    "2. How about an $m$-gram model for any $m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b1c990-f7a2-4473-b84a-9648c70b4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppl(eval_data : Iterable[str], model) -> float:\n",
    "    # TODO compute perplexity!\n",
    "    return 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
